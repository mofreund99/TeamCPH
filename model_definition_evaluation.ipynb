{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "\n",
    "# Import models you're considering\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "[For this revenue forecasting task, the most relevant model families are linear regression (high interpretability, strong baseline), tree ensembles like Random Forest/GBM (excellent for tabular nonlinear interactions), and neural networks (good when many engineered features interact in complex ways). We selected a fast-foward neural network (Keras Sequential MLP), defined with Dense hidden layers and Batch Normalization, as the primarz model. We prefered that model, because the feautres are tabular, but interaction heavy (e.g. calendar and weather effects), and MLP can learn nonlinear cross effects better than a linear model.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[## Feature Engineering\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "Following `MF_20251113.ipynb`, we use features that are available at day/product-group level and strongly related to demand:\n",
    "\n",
    "- **Calendar effects**: `IsWeekend`, `IsNewYears`, `IsHalloween`\n",
    "- **Seasonality (Fourier terms)**: `sin_1y`, `cos_1y`, `sin_2y`, `cos_2y`\n",
    "- **Event/holiday indicators**: `holiday`, `Easter`, `KielerWoche`\n",
    "- **Autoregressive signals**: `Revenue_lag1`, `Revenue_lag7` (within each `Warengruppe`)\n",
    "- **Product-group fixed effects**: one-hot encoded `Warengruppe` dummies\n",
    "\n",
    "These features provide a strong, interpretable baseline before moving to more complex models.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7433, 17)\n",
      "Test shape: (1859, 17)\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data (aligned with MF_20251113.ipynb)\n",
    "sales = pd.read_csv('/workspaces/TeamCPH/data/umsatzdaten_gekuerzt.csv', parse_dates=['Datum'])\n",
    "wetter = pd.read_csv('/workspaces/TeamCPH/data/wetter1.csv', parse_dates=['Datum'])\n",
    "kiwo = pd.read_csv('/workspaces/TeamCPH/data/kiwo.csv', parse_dates=['Datum'])\n",
    "holidays = pd.read_csv('/workspaces/TeamCPH/data/school_holidays_SH.csv', parse_dates=['Datum'])\n",
    "\n",
    "# Aggregate to daily revenue per product group\n",
    "sales_daily = (\n",
    "    sales\n",
    "    .groupby(['Datum', 'Warengruppe'], as_index=False)['Umsatz']\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# Merge exogenous data\n",
    "merged = sales_daily.merge(wetter, on='Datum', how='left')\n",
    "merged = merged.merge(kiwo, on='Datum', how='left')\n",
    "merged = merged.merge(holidays, on='Datum', how='left')\n",
    "\n",
    "# Ensure expected indicator columns exist\n",
    "for col in ['holiday', 'Easter']:\n",
    "    if col not in merged.columns:\n",
    "        merged[col] = 0\n",
    "\n",
    "if 'KielerWoche' in merged.columns:\n",
    "    merged['KielerWoche'] = merged['KielerWoche'].fillna(0).astype(int)\n",
    "else:\n",
    "    merged['KielerWoche'] = 0\n",
    "\n",
    "# Calendar features\n",
    "merged = merged.sort_values(['Warengruppe', 'Datum']).reset_index(drop=True)\n",
    "merged['IsWeekend'] = merged['Datum'].dt.weekday.isin([5, 6]).astype(int)\n",
    "merged['IsNewYears'] = (merged['Datum'].dt.strftime('%m-%d') == '12-31').astype(int)\n",
    "halloween_days = [f'10-{day:02d}' for day in range(24, 32)]\n",
    "merged['IsHalloween'] = merged['Datum'].dt.strftime('%m-%d').isin(halloween_days).astype(int)\n",
    "\n",
    "# Seasonality features (Fourier terms)\n",
    "merged['DayOfYear'] = merged['Datum'].dt.dayofyear\n",
    "merged['sin_1y'] = np.sin(2 * np.pi * merged['DayOfYear'] / 365.25)\n",
    "merged['cos_1y'] = np.cos(2 * np.pi * merged['DayOfYear'] / 365.25)\n",
    "merged['sin_2y'] = np.sin(4 * np.pi * merged['DayOfYear'] / 365.25)\n",
    "merged['cos_2y'] = np.cos(4 * np.pi * merged['DayOfYear'] / 365.25)\n",
    "\n",
    "# Lag features within product group\n",
    "merged['Revenue_lag1'] = merged.groupby('Warengruppe')['Umsatz'].shift(1)\n",
    "merged['Revenue_lag7'] = merged.groupby('Warengruppe')['Umsatz'].shift(7)\n",
    "\n",
    "# Product-group dummies\n",
    "wg_dummies = pd.get_dummies(merged['Warengruppe'], prefix='WG', drop_first=True, dtype=int)\n",
    "merged = pd.concat([merged, wg_dummies], axis=1)\n",
    "\n",
    "# Define predictors\n",
    "predictors = [\n",
    "    'holiday', 'Easter', 'KielerWoche',\n",
    "    'IsWeekend', 'IsNewYears', 'IsHalloween',\n",
    "    'sin_1y', 'cos_1y', 'sin_2y', 'cos_2y',\n",
    "    'Revenue_lag1', 'Revenue_lag7',\n",
    "] + wg_dummies.columns.tolist()\n",
    "\n",
    "# Build modeling table and split\n",
    "model_df = merged[['Umsatz'] + predictors].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "X = model_df[predictors]\n",
    "y = np.log1p(model_df['Umsatz'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "print('Train shape:', X_train.shape)\n",
    "print('Test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "[Discuss any hyperparameter tuning methods you've applied, such as Grid Search or Random Search, and the rationale behind them.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement hyperparameter tuning\n",
    "# Example using GridSearchCV with a DecisionTreeClassifier\n",
    "# param_grid = {'max_depth': [2, 4, 6, 8]}\n",
    "# grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "[Implement the final model(s) you've selected based on the above steps.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-24 19:59:55.310923: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-24 19:59:56.987697: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-24 20:00:01.446221: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 53609.8555 - val_loss: 51057.2188\n",
      "Epoch 2/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 35961.2031 - val_loss: 19382.1230\n",
      "Epoch 3/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 13803.5625 - val_loss: 10415.2139\n",
      "Epoch 4/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8813.1094 - val_loss: 7293.7354\n",
      "Epoch 5/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 6721.8071 - val_loss: 6413.4849\n",
      "Epoch 6/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 6081.5562 - val_loss: 6084.4531\n",
      "Epoch 7/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 5808.1504 - val_loss: 5900.5698\n",
      "Epoch 8/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 5710.5884 - val_loss: 5717.9404\n",
      "Epoch 9/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 5511.2075 - val_loss: 5434.3945\n",
      "Epoch 10/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5298.9395 - val_loss: 5211.6123\n",
      "Epoch 11/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 5234.6118 - val_loss: 4995.6440\n",
      "Epoch 12/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 4977.9766 - val_loss: 4800.2886\n",
      "Epoch 13/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4707.7041 - val_loss: 4665.3755\n",
      "Epoch 14/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4628.0586 - val_loss: 4547.2231\n",
      "Epoch 15/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 4499.9995 - val_loss: 4486.8418\n",
      "Epoch 16/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4307.2095 - val_loss: 4359.1499\n",
      "Epoch 17/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 4387.9189 - val_loss: 4306.0088\n",
      "Epoch 18/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4177.6509 - val_loss: 4160.2021\n",
      "Epoch 19/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4186.1401 - val_loss: 4128.9531\n",
      "Epoch 20/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4136.5435 - val_loss: 4049.5222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Implement the final model (MLP from MF_neural_net_estimation)\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load prepared features/labels from pickle files\n",
    "subdirectory = \"pickle_data\"\n",
    "training_features = pd.read_pickle(f\"{subdirectory}/training_features.pkl\")\n",
    "validation_features = pd.read_pickle(f\"{subdirectory}/validation_features.pkl\")\n",
    "training_labels = pd.read_pickle(f\"{subdirectory}/training_labels.pkl\")\n",
    "validation_labels = pd.read_pickle(f\"{subdirectory}/validation_labels.pkl\")\n",
    "\n",
    "model = Sequential([\n",
    "    InputLayer(shape=(training_features.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    Dense(4, activation=\"relu\"),\n",
    "    Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "history = model.fit(\n",
    "    training_features,\n",
    "    training_labels,\n",
    "    epochs=20,\n",
    "    validation_data=(validation_features, validation_labels),\n",
    ")\n",
    "\n",
    "model.save(\"python_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We evaluate performance using Mean Absolute Percentage Error (MAPE) and Mean Squared Error (MSE). MAPE is the primary metric because it is scale-free and easy to interpret as a percent error across product groups and time. MSE is tracked during training because it aligns with the loss function used to optimize the neural network and is sensitive to larger errors, which helps penalize large revenue misses. We report metrics on both training and validation sets to monitor generalization and detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step\n",
      "MAPE on the Training Data: 6957365970.34%\n",
      "MAPE on the Validation Data: 4494038732.43%\n",
      "\n",
      "MAPE per product category (Warengruppe):\n",
      "   Warengruppe  MAPE_Validation\n",
      "0            1     4.494039e+09\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using your chosen metrics\n",
    "# Example for classification\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example for regression\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Your evaluation code here\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# MAPE FUNCTION (SAFE VERSION)\n",
    "# -----------------------------\n",
    "def mape(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return 100.0 * np.mean(np.abs(y_true - y_pred) / denom)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PREDICTIONS\n",
    "# -----------------------------\n",
    "training_predictions = model.predict(training_features).reshape(-1)\n",
    "validation_predictions = model.predict(validation_features).reshape(-1)\n",
    "\n",
    "training_labels = np.asarray(training_labels).reshape(-1)\n",
    "validation_labels = np.asarray(validation_labels).reshape(-1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# OVERALL MAPE\n",
    "# -----------------------------\n",
    "print(f\"MAPE on the Training Data: {mape(training_labels, training_predictions):.2f}%\")\n",
    "print(f\"MAPE on the Validation Data: {mape(validation_labels, validation_predictions):.2f}%\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# RECONSTRUCT WARENGRUPPE FROM DUMMIES\n",
    "# WG_2 ... WG_6 exist, WG_1 is the reference group\n",
    "# -----------------------------\n",
    "def reconstruct_wg(df):\n",
    "    wg_cols = [c for c in df.columns if c.startswith(\"WG_\")]\n",
    "    \n",
    "    # Default group = 1 (reference)\n",
    "    wg = np.ones(len(df), dtype=int)\n",
    "    \n",
    "    for col in wg_cols:\n",
    "        group_number = int(col.split(\"_\")[1])\n",
    "        wg[df[col] == 1] = group_number\n",
    "    \n",
    "    return wg\n",
    "\n",
    "\n",
    "training_wg = reconstruct_wg(training_features)\n",
    "validation_wg = reconstruct_wg(validation_features)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# BUILD DATAFRAMES FOR GROUPED EVALUATION\n",
    "# -----------------------------\n",
    "train_df = pd.DataFrame({\n",
    "    \"Warengruppe\": training_wg,\n",
    "    \"y_true\": training_labels,\n",
    "    \"y_pred\": training_predictions\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    \"Warengruppe\": validation_wg,\n",
    "    \"y_true\": validation_labels,\n",
    "    \"y_pred\": validation_predictions\n",
    "})\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MAPE PER WARENGRUPPE\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "mape_val_wg = (\n",
    "    val_df\n",
    "    .groupby(\"Warengruppe\")\n",
    "    .apply(lambda x: mape(x[\"y_true\"], x[\"y_pred\"]))\n",
    "    .reset_index(name=\"MAPE_Validation\")\n",
    ")\n",
    "\n",
    "mape_per_wg = (\n",
    "    mape_val_wg.sort_values(\"Warengruppe\")\n",
    ")\n",
    "\n",
    "print(\"\\nMAPE per product category (Warengruppe):\")\n",
    "print(mape_per_wg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "We compare the pooled linear regression in MF_20251113 with the MLP neural network from MF_neural_net_estimation. The linear model is a log1p-OLS with calendar effects (weekend, holidays, Kieler Woche), Fourier seasonality terms, lagged revenue features, and Warengruppe dummies, plus a group-wise bias correction after back-transforming predictions. This gives strong interpretability (coefficients by feature) and stable behavior, but it is limited to additive linear effects in log space and may miss higher-order interactions.\n",
    "\n",
    "The neural network uses the same prepared feature set (stored in pickle files) and learns nonlinear interactions through hidden layers and batch normalization. It is typically more flexible for complex, interaction-heavy effects (e.g., weather x calendar x product group), but it is less interpretable and more sensitive to data scaling and training choices. We evaluate both models with MAPE on training/validation and can also compare MSE to align with the NN loss. The final choice balances interpretability and operational stability (linear model) against potential accuracy gains from nonlinear effects (neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "                                  Model                    Dataset      MAE  \\\n",
      "0  Baseline Linear (Temp + KielerWoche)  Baseline test split (20%)  105.101   \n",
      "1                      Neural Net (MLP)      Pickle validation set   36.624   \n",
      "\n",
      "      RMSE        MAPE_%        MSE  \n",
      "0  147.101  6.382900e+01  21638.589  \n",
      "1   63.636  9.902875e+09   4049.522  \n"
     ]
    }
   ],
   "source": [
    "# Comparative analysis: baseline_model (Temperatur + KielerWoche) vs neural net\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def mape(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return 100.0 * np.mean(np.abs(y_true - y_pred) / denom)\n",
    "\n",
    "# -----------------------------\n",
    "# Baseline model from baseline_model.ipynb\n",
    "# Umsatz ~ Temperatur + KielerWoche\n",
    "# -----------------------------\n",
    "sales = pd.read_csv('/workspaces/TeamCPH/data/umsatzdaten_gekuerzt.csv', parse_dates=['Datum'])\n",
    "wetter = pd.read_csv('/workspaces/TeamCPH/data/wetter1.csv', parse_dates=['Datum'])\n",
    "kiwo = pd.read_csv('/workspaces/TeamCPH/data/kiwo.csv', parse_dates=['Datum'])\n",
    "\n",
    "sales_daily = (\n",
    "    sales\n",
    "    .groupby(['Datum', 'Warengruppe'], as_index=False)['Umsatz']\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "baseline_df = sales_daily.merge(wetter, on='Datum', how='left')\n",
    "baseline_df = baseline_df.merge(kiwo, on='Datum', how='left')\n",
    "\n",
    "if 'KielerWoche' in baseline_df.columns:\n",
    "    baseline_df['KielerWoche'] = baseline_df['KielerWoche'].fillna(0).astype(int)\n",
    "else:\n",
    "    baseline_df['KielerWoche'] = 0\n",
    "\n",
    "baseline_features = ['Temperatur', 'KielerWoche']\n",
    "baseline_df = baseline_df[['Umsatz'] + baseline_features].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "X_base = baseline_df[baseline_features]\n",
    "y_base_log = np.log1p(baseline_df['Umsatz'])\n",
    "\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(\n",
    "    X_base, y_base_log, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(X_train_b, y_train_b)\n",
    "\n",
    "y_pred_base_log = baseline_model.predict(X_test_b)\n",
    "y_pred_base = np.expm1(y_pred_base_log)\n",
    "y_true_base = np.expm1(y_test_b)\n",
    "\n",
    "mae_base = mean_absolute_error(y_true_base, y_pred_base)\n",
    "rmse_base = np.sqrt(mean_squared_error(y_true_base, y_pred_base))\n",
    "mape_base = mape(y_true_base, y_pred_base)\n",
    "mse_base = mean_squared_error(y_true_base, y_pred_base)\n",
    "\n",
    "# -----------------------------\n",
    "# Neural net (saved model + validation pickle files)\n",
    "# -----------------------------\n",
    "subdirectory = 'pickle_data'\n",
    "validation_features = pd.read_pickle(f'{subdirectory}/validation_features.pkl')\n",
    "validation_labels = pd.read_pickle(f'{subdirectory}/validation_labels.pkl')\n",
    "\n",
    "nn_model = load_model('python_model.h5', compile=False)\n",
    "nn_model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "nn_pred = nn_model.predict(validation_features).reshape(-1)\n",
    "nn_true = np.asarray(validation_labels).reshape(-1)\n",
    "\n",
    "mae_nn = mean_absolute_error(nn_true, nn_pred)\n",
    "rmse_nn = np.sqrt(mean_squared_error(nn_true, nn_pred))\n",
    "mape_nn = mape(nn_true, nn_pred)\n",
    "mse_nn = mean_squared_error(nn_true, nn_pred)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Baseline Linear (Temp + KielerWoche)', 'Neural Net (MLP)'],\n",
    "    'Dataset': ['Baseline test split (20%)', 'Pickle validation set'],\n",
    "    'MAE': [mae_base, mae_nn],\n",
    "    'RMSE': [rmse_base, rmse_nn],\n",
    "    'MAPE_%': [mape_base, mape_nn],\n",
    "    'MSE': [mse_base, mse_nn],\n",
    "})\n",
    "\n",
    "print(comparison.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
