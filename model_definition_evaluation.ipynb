{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "# Import models you're considering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "[For this revenue forecasting task, the most relevant model families are linear regression (high interpretability, strong baseline), tree ensembles like Random Forest/GBM (excellent for tabular nonlinear interactions), and neural networks (good when many engineered features interact in complex ways). We selected a fast-foward neural network (Keras Sequential MLP), defined with Dense hidden layers and Batch Normalization, as the primarz model. We prefered that model, because the feautres are tabular, but interaction heavy (e.g. calendar and weather effects), and MLP can learn nonlinear cross effects better than a linear model.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "[Describe any additional feature engineering you've performed beyond what was done for the baseline model.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Replace 'your_dataset.csv' with the path to your actual dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43myour_dataset.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Perform any feature engineering steps\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Example: df['new_feature'] = df['feature1'] + df['feature2']\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Feature and target variable selection\u001b[39;00m\n\u001b[32m      9\u001b[39m X = df[[\u001b[33m'\u001b[39m\u001b[33myour\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mselected\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/io/parsers/readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/io/parsers/readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/io/common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'your_dataset.csv'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "# Replace 'your_dataset.csv' with the path to your actual dataset\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Perform any feature engineering steps\n",
    "# Example: df['new_feature'] = df['feature1'] + df['feature2']\n",
    "\n",
    "# Feature and target variable selection\n",
    "X = df[['your', 'selected', 'features']]\n",
    "y = df['target_variable']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "[Discuss any hyperparameter tuning methods you've applied, such as Grid Search or Random Search, and the rationale behind them.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement hyperparameter tuning\n",
    "# Example using GridSearchCV with a DecisionTreeClassifier\n",
    "# param_grid = {'max_depth': [2, 4, 6, 8]}\n",
    "# grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "[Implement the final model(s) you've selected based on the above steps.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-23 18:29:51.521992: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-02-23 18:29:51.549483: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-23 18:29:53.225268: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-23 18:29:56.117744: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-23 18:29:56.118809: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-23 18:29:58.201608: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 53055.5273 - val_loss: 49176.6992\n",
      "Epoch 2/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 29690.0840 - val_loss: 16204.8867\n",
      "Epoch 3/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12076.5068 - val_loss: 10658.3516\n",
      "Epoch 4/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8096.8789 - val_loss: 7847.6914\n",
      "Epoch 5/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6660.0972 - val_loss: 7088.0107\n",
      "Epoch 6/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6033.3896 - val_loss: 6798.0093\n",
      "Epoch 7/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5964.6709 - val_loss: 6641.3784\n",
      "Epoch 8/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5831.7998 - val_loss: 6468.5229\n",
      "Epoch 9/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5705.5537 - val_loss: 6354.0884\n",
      "Epoch 10/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5542.4097 - val_loss: 6231.5029\n",
      "Epoch 11/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5502.5547 - val_loss: 6122.3613\n",
      "Epoch 12/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5372.3096 - val_loss: 5971.6401\n",
      "Epoch 13/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5286.0581 - val_loss: 5847.7378\n",
      "Epoch 14/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5263.3687 - val_loss: 5740.7188\n",
      "Epoch 15/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5211.9712 - val_loss: 5620.7856\n",
      "Epoch 16/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5195.7573 - val_loss: 5512.4595\n",
      "Epoch 17/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5009.6167 - val_loss: 5416.4058\n",
      "Epoch 18/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5009.9673 - val_loss: 5385.3521\n",
      "Epoch 19/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4936.5635 - val_loss: 5268.0420\n",
      "Epoch 20/20\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4893.2036 - val_loss: 5245.8237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Implement the final model (MLP from MF_neural_net_estimation)\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load prepared features/labels from pickle files\n",
    "subdirectory = \"pickle_data\"\n",
    "training_features = pd.read_pickle(f\"{subdirectory}/training_features.pkl\")\n",
    "validation_features = pd.read_pickle(f\"{subdirectory}/validation_features.pkl\")\n",
    "training_labels = pd.read_pickle(f\"{subdirectory}/training_labels.pkl\")\n",
    "validation_labels = pd.read_pickle(f\"{subdirectory}/validation_labels.pkl\")\n",
    "\n",
    "model = Sequential([\n",
    "    InputLayer(shape=(training_features.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    Dense(4, activation=\"relu\"),\n",
    "    Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "history = model.fit(\n",
    "    training_features,\n",
    "    training_labels,\n",
    "    epochs=20,\n",
    "    validation_data=(validation_features, validation_labels),\n",
    ")\n",
    "\n",
    "model.save(\"python_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We evaluate performance using Mean Absolute Percentage Error (MAPE) and Mean Squared Error (MSE). MAPE is the primary metric because it is scale-free and easy to interpret as a percent error across product groups and time. MSE is tracked during training because it aligns with the loss function used to optimize the neural network and is sensitive to larger errors, which helps penalize large revenue misses. We report metrics on both training and validation sets to monitor generalization and detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step\n",
      "MAPE on the Training Data: 6957365970.34%\n",
      "MAPE on the Validation Data: 4494038732.43%\n",
      "\n",
      "MAPE per product category (Warengruppe):\n",
      "   Warengruppe  MAPE_Validation\n",
      "0            1     4.494039e+09\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using your chosen metrics\n",
    "# Example for classification\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example for regression\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Your evaluation code here\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# MAPE FUNCTION (SAFE VERSION)\n",
    "# -----------------------------\n",
    "def mape(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return 100.0 * np.mean(np.abs(y_true - y_pred) / denom)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PREDICTIONS\n",
    "# -----------------------------\n",
    "training_predictions = model.predict(training_features).reshape(-1)\n",
    "validation_predictions = model.predict(validation_features).reshape(-1)\n",
    "\n",
    "training_labels = np.asarray(training_labels).reshape(-1)\n",
    "validation_labels = np.asarray(validation_labels).reshape(-1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# OVERALL MAPE\n",
    "# -----------------------------\n",
    "print(f\"MAPE on the Training Data: {mape(training_labels, training_predictions):.2f}%\")\n",
    "print(f\"MAPE on the Validation Data: {mape(validation_labels, validation_predictions):.2f}%\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# RECONSTRUCT WARENGRUPPE FROM DUMMIES\n",
    "# WG_2 ... WG_6 exist, WG_1 is the reference group\n",
    "# -----------------------------\n",
    "def reconstruct_wg(df):\n",
    "    wg_cols = [c for c in df.columns if c.startswith(\"WG_\")]\n",
    "    \n",
    "    # Default group = 1 (reference)\n",
    "    wg = np.ones(len(df), dtype=int)\n",
    "    \n",
    "    for col in wg_cols:\n",
    "        group_number = int(col.split(\"_\")[1])\n",
    "        wg[df[col] == 1] = group_number\n",
    "    \n",
    "    return wg\n",
    "\n",
    "\n",
    "training_wg = reconstruct_wg(training_features)\n",
    "validation_wg = reconstruct_wg(validation_features)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# BUILD DATAFRAMES FOR GROUPED EVALUATION\n",
    "# -----------------------------\n",
    "train_df = pd.DataFrame({\n",
    "    \"Warengruppe\": training_wg,\n",
    "    \"y_true\": training_labels,\n",
    "    \"y_pred\": training_predictions\n",
    "})\n",
    "\n",
    "val_df = pd.DataFrame({\n",
    "    \"Warengruppe\": validation_wg,\n",
    "    \"y_true\": validation_labels,\n",
    "    \"y_pred\": validation_predictions\n",
    "})\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MAPE PER WARENGRUPPE\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "mape_val_wg = (\n",
    "    val_df\n",
    "    .groupby(\"Warengruppe\")\n",
    "    .apply(lambda x: mape(x[\"y_true\"], x[\"y_pred\"]))\n",
    "    .reset_index(name=\"MAPE_Validation\")\n",
    ")\n",
    "\n",
    "mape_per_wg = (\n",
    "    mape_val_wg.sort_values(\"Warengruppe\")\n",
    ")\n",
    "\n",
    "print(\"\\nMAPE per product category (Warengruppe):\")\n",
    "print(mape_per_wg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "We compare the pooled linear regression in MF_20251113 with the MLP neural network from MF_neural_net_estimation. The linear model is a log1p-OLS with calendar effects (weekend, holidays, Kieler Woche), Fourier seasonality terms, lagged revenue features, and Warengruppe dummies, plus a group-wise bias correction after back-transforming predictions. This gives strong interpretability (coefficients by feature) and stable behavior, but it is limited to additive linear effects in log space and may miss higher-order interactions.\n",
    "\n",
    "The neural network uses the same prepared feature set (stored in pickle files) and learns nonlinear interactions through hidden layers and batch normalization. It is typically more flexible for complex, interaction-heavy effects (e.g., weather x calendar x product group), but it is less interpretable and more sensitive to data scaling and training choices. We evaluate both models with MAPE on training/validation and can also compare MSE to align with the NN loss. The final choice balances interpretability and operational stability (linear model) against potential accuracy gains from nonlinear effects (neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "                       Model  MAPE_Validation  MSE_Validation\n",
      "0  Linear Regression (log1p)     2.213580e+01     4498.273482\n",
      "1           Neural Net (MLP)     4.494039e+09     5245.824342\n"
     ]
    }
   ],
   "source": [
    "# Comparative analysis: linear regression (MF_20251113-style) vs neural net (MF_neural_net_estimation)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def mape(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return 100.0 * np.mean(np.abs(y_true - y_pred) / denom)\n",
    "\n",
    "# -----------------------------\n",
    "# Load tabular features used by the linear regression\n",
    "# -----------------------------\n",
    "model_df = pd.read_csv(\"model_df.csv\")\n",
    "model_df = model_df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "exclude_cols = {\"Umsatz\", \"Warengruppe\", \"log_Umsatz\"}\n",
    "predictors = [c for c in model_df.columns if c not in exclude_cols]\n",
    "\n",
    "# Reproduce the same split procedure (seed=42, shuffle, 70/20/10)\n",
    "model_df = model_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "n_total = len(model_df)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.20 * n_total)\n",
    "\n",
    "train_df = model_df.iloc[:n_train]\n",
    "val_df = model_df.iloc[n_train:n_train + n_val]\n",
    "\n",
    "X_train = train_df[predictors]\n",
    "X_val = val_df[predictors]\n",
    "y_train_log = np.log1p(train_df[\"Umsatz\"])\n",
    "y_val = val_df[\"Umsatz\"].to_numpy()\n",
    "\n",
    "# Fit linear regression on log1p(Umsatz) like MF_20251113\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    X_train_sm = sm.add_constant(X_train)\n",
    "    X_val_sm = sm.add_constant(X_val, has_constant=\"add\")\n",
    "    results = sm.OLS(y_train_log, X_train_sm).fit()\n",
    "    y_pred_log = results.predict(X_val_sm)\n",
    "except ModuleNotFoundError:\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train_log)\n",
    "    y_pred_log = lr.predict(X_val)\n",
    "\n",
    "y_pred_lr = np.expm1(y_pred_log)\n",
    "mape_lr = mape(y_val, y_pred_lr)\n",
    "mse_lr = mean_squared_error(y_val, y_pred_lr)\n",
    "\n",
    "# -----------------------------\n",
    "# Neural network predictions on the same validation split (pickle files)\n",
    "# -----------------------------\n",
    "subdirectory = \"pickle_data\"\n",
    "validation_features = pd.read_pickle(f\"{subdirectory}/validation_features.pkl\")\n",
    "validation_labels = pd.read_pickle(f\"{subdirectory}/validation_labels.pkl\")\n",
    "\n",
    "# Load without compilation to avoid legacy H5 metric deserialization issues\n",
    "nn_model = load_model(\"python_model.h5\", compile=False)\n",
    "nn_model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "nn_pred_val = nn_model.predict(validation_features).reshape(-1)\n",
    "nn_true_val = np.asarray(validation_labels).reshape(-1)\n",
    "\n",
    "mape_nn = mape(nn_true_val, nn_pred_val)\n",
    "mse_nn = mean_squared_error(nn_true_val, nn_pred_val)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Model\": [\"Linear Regression (log1p)\", \"Neural Net (MLP)\"],\n",
    "    \"MAPE_Validation\": [mape_lr, mape_nn],\n",
    "    \"MSE_Validation\": [mse_lr, mse_nn],\n",
    "})\n",
    "\n",
    "print(comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
